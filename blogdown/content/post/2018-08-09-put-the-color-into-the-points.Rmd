---
title: 'LD Part 1'
subtitle: 'Access precomputed LD'
date: '2019-05-21'
slug: get-ld-remotely
code_download: true
tags: 
  - statistical genetics
  - R
  - data visualisation
draft: true
header:
  caption: ''
  image: ''
output:
  blogdown::html_page:
    toc: true
---

```{r, setup, include = FALSE, warning=FALSE, message=FALSE}
#library(showtext)
suppressPackageStartupMessages(library(tidyverse))
library(ggplot2)
library(httr)
library(jsonlite)
library(xml2)
library(tibble)
library(tidyr)
library(magrittr)
theme_set(theme_bw())  ## make all ggplots white-ish
```


The squared correlation between genetic markers is one way to estimate [linkage disequilibrium](https://ldlink.nci.nih.gov/?tab=help) (LD). LD has to be computed *all the time* - either for as an input for statistical methods or to summarise results. 

However, accessing LD estimations quickly, for a specific population and in an automated way (e.g. with R) is actually suprisingly difficult. 

In this blogpost I am exploring how to do this efficiently.


<!---------- ------------------------------->
## Goal
<!---------- ------------------------------->

At the end of this blogpost, we want to know the genetic correlation between two or more markers in a specific human population, so we can populate the locuszoom plot from the [previous blogpost](https://sinarueeger.github.io/2018/07/30/locuszoomplot/) with colored dots. 

For simplicity, I will use the terms correlation, squared correlation, r, r2 and LD interchangeably. 

<!---------- ------------------------------->
## Two approaches
<!---------- ------------------------------->

In principal, there are two ways of doing accessing LD: 

1. Download (or access) the genetic data from which you want to estimate your correlations + calculate the correlations using some efficient approach. 
2. Access precomputed LD estimations.  


| Approach `r icon::fa_wrench()`                 	  | Advantages `r icon::fa_smile()`                                                 	| Downsides  `r icon::fa_frown()`                                                                                                    	| Useful when...                                                                  	|
|-------------------------	  |-------------------------------------------------------------	|--------------------------------------------------------------------------------------------------------------	|--------------------------------------------------------------------------------	|
| (1) **Local** computation of LD 	  | LD matrix can be quickly updated to new reference panels 	| Requires large computation and storage space (e.g. 1000 Genomes is [>100 few GB large](https://console.cloud.google.com/storage/browser/genomics-public-data/1000-genomes-phase-3/vcf/?_ga=2.227457240.-473641130.1558969048)). 	|  i) LD for a large set of SNPs is needed ii) LD from non-standard reference panel is needed. 	|
| (2) **Access** precomputed LD   	  | not need for large computation and storage space.       	| limited to certain small sets of markers, limited to possibly outdated reference panels.                                        	| LD for a small set of SNPs is needed                              	|

For now I will focus on approach (2), and then explore approach (1) in a future blogpost. 

Spoiler: Using approach (2) does not get you far. It took me quite a while to gather all the solutions that are listed below, and yet there is not one perfect/ideal solution. 


<!---------- ------------------------------->
## Our toy data
<!---------- ------------------------------->

We will recycle the data from the [previous blogpost](https://sinarueeger.github.io/2018/07/30/locuszoomplot/), where the focus was on extracting annotation using the package `biomaRt`. In this blogpost, we will complete that locuszoomplot by adding the LD information. 

```{r, load-data, echo=TRUE, results="hide", cache=TRUE, message = FALSE, warning = FALSE}

## Data Source URL
url <- "https://portals.broadinstitute.org/collaboration/giant/images/2/21/BMI_All_ancestry.fmt.gzip"

## Import BMI summary statistics dat.bmi <- read_tsv(file = url) ##
## taking too long, let's use fread instead.

dat.bmi <- data.table::fread(url, verbose = FALSE)

## Rename some columns
dat.bmi <- dat.bmi %>% rename(SNP = SNPNAME, P = Pvalue)

## Extract region
dat.bmi.sel <- dat.bmi %>% slice(which.min(P))
dat.bmi.sel

## range region
range <- 5e+05
sel.chr <- dat.bmi.sel$CHR
sel.pos <- dat.bmi.sel$POS

dat.bmi.sel.region <- dat.bmi %>% 
  filter(CHR == sel.chr, between(POS, sel.pos - 
      range, sel.pos + range))

```

What we are interested in, is the LD between our top SNP `r dat.bmi.sel$SNP` and all remaining SNPs. 

Remember that this dataset has positions on build GRCh37, while most databases are on build GRCh38 by now. 

```{r, look-up-position}
library(rsnps)
snp <- dat.bmi.sel$SNP
sm <- ncbi_snp_summary(snp) %>% separate(chrpos, c("chr", "pos"))
sel.pos == as.numeric(sm$pos)

```



<!---------- ------------------------------->
## A solution that almost works: ensembl.org
<!---------- ------------------------------->

Let's quickly repeat what our primary goal is:

Extract the correlation between SNPs

- without downloading any data,
- fairly quick and
- in R.

Let's first define our top SNP and the range in kb around that SNP for which we want LD information.

```{r, prep}
snp <- dat.bmi.sel$SNP
range.kb <- range/1e3 ## in k bp

```

The [REST API of Ensembl](https://rest.ensembl.org) can do a lot (see options [here](https://rest.ensembl.org/documentation/)). For example: access precomputed LD. The webpage even provides R code to do so, which is from where I copied some snippets below. 

To access the rest API at ensembl, we need the following three packages loaded.

```{r, prep-packages-ensembl}
library(httr)
library(jsonlite)
library(xml2)
library(tibble)
library(tidyr)
library(magrittr)
```

### What reference panels / population can we choose from? 
<!------------------------------>

Currently, the largest and hence most popular public reference panel is [1000 Genomes reference panel](http://www.internationalgenome.org/) (1KG). The 26 populations of roughly 100 individuals each can be grouped into five super populations: African (AFR), American (AMR), European (EUR), South Asian (SAS), East Asian (EAS). 

We can ask the ENSEMBL API from what populations reference panels are available. This will return us a dataframe.

```{r, populations}

 
server <- "https://rest.ensembl.org"
ext <- "/info/variation/populations/homo_sapiens?filter=LD"
 
r <- GET(paste(server, ext, sep = ""), content_type("application/json"))
 
stop_for_status(r)
 
head(fromJSON(toJSON(content(r))))

```
`name` stands for the population identifier. `size` refers to the number of individuals in the reference panel. Note that these are all populations with around 100 individuals (the correlation estimation will have an error that scales with the sample size). There are also the five super population available (although not listed here), simply replace the last three characters in `name` by `EUR`, `AFR`, `AMR`, `EAS`, `SAS`.  

<iframe seamless src="/post/2018-08-09-put-the-color-into-the-points/map-1000genomes-populations.html" width="100%" height="500"></iframe> <sup>(From <a href="../1kgmap.html">this</a> blogpost.)</sup>

We want the LD information, so we can add this info to the locuszoom plot. But how do we know which population to pick? One way is, to read up what kind of individuals were present. In our case - mostly Europeans (`EUR`). But we could also build some pooled LD matrix of different populations. 

Now that we know which reference panel we want to use, we can use the different rest APIs. 

- [Access LD between a SNP and its region](#access-ld-between-a-snp-and-its-region)
- [Access LD matrix](#access-ld-matrix)
- [Access LD between a SNP and many other SNPs](#access-ld-between-a-snp-and-many-other-snps)

### Access LD between a SNP and its region
<!------------------------------>

This API is described [here](https://rest.ensembl.org/documentation/info/ld_id_get).

The default window size is 500 kb. There are also thresholds for `r2` (e.g. if you want to filter all SNPs with an `r2 > 0.8`).

The only input required is the SNP rsid, marked with `{snp}`. 

```{r echo=TRUE}
snp
server <- "https://rest.ensembl.org"
ext <- glue::glue("/ld/human/{snp}/1000GENOMES:phase_3:EUR?")
## Window size in kb. The maximum allowed value for the window size is 500 kb. LD is computed for the given variant and all variants that are located within the specified window.

r <- GET(paste(server, ext, sep = ""), content_type("application/json"))
stop_for_status(r)
LD.SNP.region <- as_tibble(fromJSON(toJSON(content(r)))) %>% unnest() %>% mutate(r2 = as.numeric(r2))
head(LD.SNP.region)

```

As a result, `LD.snp.region` contains the `r2` of our top SNP with all SNPs that were +/- 500 kb away. 

What if we want the correlation between all SNPs? 

### Access LD matrix
<!------------------------------>

For this we need the rest API [here](https://rest.ensembl.org/documentation/info/ld_region_get).

We can calculate the LD matrix of a full region, max 1 Mb wide. For fast computation we limit it to +/- 100 kb. 

```{r, calc-ld-matrix, eval=FALSE, cache=TRUE, include=FALSE}
## Query region. A maximum of 1Mb is allowed.

ext <- glue::glue("/ld/human/region/{sel.chr}:{sel.pos - range/10}..{sel.pos + range/10}/1000GENOMES:phase_3:EUR?")

r <- GET(paste(server, ext, sep = ""), content_type("application/json"))
stop_for_status(r)
LD.matrix.region <- as_tibble(fromJSON(toJSON(content(r)))) %>% unnest()
LD.matrix.region

```

We can then quickly visualise our LD matrix:

```{r, plot-ld-matrix, eval=FALSE, cache=TRUE, include=FALSE, fig.height = 4, fig.width = 4}

ggplot(data = LD.matrix.region) + 
  geom_tile(aes(x = variation1, y = variation2, fill = r2)) + 
  scale_fill_gradient(low = 'white', high = "red")

```

### Access LD between a SNP and many other SNPs
<!------------------------------>

The [third and last option](https://rest.ensembl.org/documentation/info/ld_pairwise_get) is to pass on a set of SNP rs ids, and access the LD among these. Implemented in the ENSEMBL API is only the LD between two SNPs, so we will have to extend this to many SNPs. 

```{r calc-ld-SNP-SNPS, eval=FALSE, cache=TRUE, include=FALSE}

extract_ld <- function(SNP.id2 = NULL, SNP.id1 = NULL, POP = NULL)
{
  ext <- glue::glue("/ld/human/pairwise/{SNP.id1}/{SNP.id2}/")  ## filter POP further down
 
  server <- "https://rest.ensembl.org"

  r <- GET(paste(server, ext, sep = ""), content_type("application/json"))
  stop_for_status(r)

  out <- as_tibble(fromJSON(toJSON(content(r)))) %>% 
    unnest() %>% 
    filter(stringr::str_detect(population_name, POP))
  
  return(out)
}


## see futher down why intersect here
other.snps <- intersect(LD.SNP.region$variation2, dat.bmi.sel.region$SNP) 

## cacluate LD for all other.snps SNPs
LD.matrix.snps <- purrr::map_df(other.snps, extract_ld, snp, "EUR") %>% 
  mutate(r2 = as.numeric(r2)) %>% 
  bind_rows() %>% 
  unnest()
LD.matrix.snps

```

Calculate the LD matrix (`LD.matrix.region`) or the LD between SNP pairs (`LD.matrix.snps`) takes some time. 

### Colored locuszoom plot
<!---------- ------------------------------->

For the locuszoomplot we need only the correlation between the top SNP and all other SNPs. So we join the object `LD.SNP.region` to `dat.bmi.sel.region`. 

```{r, join-ld}
dat.bmi.plot <- dat.bmi.sel.region %>% 
  full_join(LD.SNP.region, by = c("SNP" = "variation2"))

```


```{r, plot-summarystats-ld, fig.height = 5, eval=FALSE, cache=TRUE, include=FALSE}

ggplot(data = dat.bmi.plot) + 
  geom_point(aes(POS, -log10(P), color = r2), shape = 16) + 
  labs(title = "Locuszoomplot for BMI GWAS", 
       subtitle = paste("Summary statistics for chromosome", sel.chr, "from", format((sel.pos - range), big.mark = "'"), "to", format((sel.pos + range), big.mark = "'"), "bp"), 
       caption = paste("Data source:", url)) +
  geom_point(data = dat.bmi.plot %>% filter(SNP == "rs1421085"), 
             aes(POS, -log10(P)), color = "black", shape = 16) + 
  scale_color_distiller(type = "div", palette = "Spectral", limits = c(0,1))

```

But wait, why are there so few SNPs in LD with this SNP? 

<!-- REST API: https://rviews.rstudio.com/2018/07/23/rest-apis-and-plumber/ ----->

<!---------- ------------------------------->
## 2. Solutions that work half-through
<!---------- ------------------------------->



### ldlink
<!------------------------------>

[ldlink](https://ldlink.nci.nih.gov/?tab=help#Programatic%20Access) is another rest API, but uses a limited set of 1KG populations. 

Here is an example, that you can execute in a terminal. 

```{bash}
curl -k -X GET 'https://ldlink.nci.nih.gov/LDlinkRest/ldmatrix?snps=rs3%0Ars4%0Ars148890987&pop=CEU&r2_d=d&token=975d104d9f13'

```

### SNPsnap
<!------------------------------>

- SNPsnap: https://data.broadinstitute.org/mpg/snpsnap/database_download.html
- uses a limited set of 1KG populations (EUR, EAS, WAFR).

### API provided by sph.umich
<!------------------------------>

- [API](http://portaldev.sph.umich.edu/docs/api/v1/?shell#single-variant-statistics)
- uses limited set of 1KG populations (ALL, EUR)
- see [github issue](https://github.com/statgen/locuszoom-api/issues/21#issuecomment-414434704))


<!---- ### Using locuszoom software ----->
<!------------------------------>

<!---- One option (or rather a hack) would be to use [locuszoom software](https://genome.sph.umich.edu/wiki/LocusZoom_Standalone) to extract their LD information. However, locuszoom comes with database and LD files, [yielding up to 23 G](https://github.com/statgen/locuszoom-standalone). Besides, its not maintained anymore, since a [new version](https://github.com/statgen/locuszoom) for [interactive usage](http://locuszoom.org/locuszoomjs.php) was developed. 

------->
<!---------- ------------------------------->
## 3. Solution that does not work
<!---------- ------------------------------->

### `rsnps::ld_search`
<!------------------------------>

A perfect solution would have been the function `ld_search` from R package [`rsnps`](ropensci webpage). It has arguments to choose the reference panel, the population, the distance from the focal SNP. 

The problem is, that it only uses old reference panels (HapMap and 1KG-phase1). Meaning, many newer reference panel populations are left out. 

But the main problem is, that the broad institute has taken down the [snap server](https://www.broadinstitute.org/snap/snap) that `ld_search` used to access (see [github issue](https://github.com/ropensci/rsnps/issues/60)), hence `ld_search` [is defunct](https://github.com/ropensci/rsnps/blob/master/R/LDSearch.R).




## Session Info

```{r, sessioninfo, include = TRUE}
sessionInfo()
```
